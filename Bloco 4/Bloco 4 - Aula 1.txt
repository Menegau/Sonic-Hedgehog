Atualizações readme:

A. Explicar brevemente cada um dos blocos trabalhados
	No Bloco 1, tratamos os dados do DataFrame escolhido. 
	1) Eliminamos espaços sem dados;
	2) Agrupamos os features numéricos;
	3) Normalizamos os dados para facilitar seu uso
	4) Descrevemos alguns fenômenos que ocorrem nos dados, tais como skewness (assimetria) e kurtosis (curtose - achatamento da curva de função de distribuição de probabilidade);
	5) Plotamos gráficos referentes aos dados analisados e à sua distribuição;
	6) Finalmente, citamos as fontes utilizadas.
	

	No Bloco 2 - Aprendizado Supervisionado -, os dados foram separados em gurpos para treino e para teste. Os métodos abordados foram os seguintes:
	1) k-NN (Nearest Neighbors ou vizinhos mais próximos). Consiste em um algoritmo que utiliza a proximidade dos dados para classificá-los em uma determinada categoria. 
	2) Regressão Linear. Consiste em prever um valor (variável dependente) por meio de outro já estabelecido (variável independente). 
	3) Árvore de decisão. Consiste em um mapa dos possíveis resultados visualizados por meio de diversas escolhas realizadas. 
	4) RandomForest (Floresta Aleatória). Consiste em um método de classificação (ou regressão) que utiliza várias árvores de decisão durante o seu treinamento.
	Após o estudo desses 4 métodos, realizamos a comparação entre eles, a fim de observar e avalariar o melhor resultado. Com isso, realizamos mais uma classíficação, afim de escolhermos os hiperparâmetros a serem utilizados no modelo de melhor resultado. 


	No Bloco 3 - Aprendizado Não Supervisionado -, foram utilizados algoritmos para analisar e clusterizar (agrupar/aglomerar) os dados não rotulados, para, dessa forma, encontrarem padrões por conta própria.
	1) O primeiro passo, foi a redução da dimensionalidade dos dados, através do método PCA, ou Principal Component Analysis.
	2) Após isso, foi realizada a clusterização dos dados através do método k-means, o qual consiste em separar os dados por meio de determinada quantidade de *centroids* (centros) de aglomerações. 
	3) Além do k-means, um outro método a nosso critério foi escolhido, o "Clustering Hierárquico".

	Com os clusters definidos, utilizamos métodos para detectar os *outliers* (indivíduos ou dados fora do comum) no DataFrame. Para isso, foram aplicados os seguintes métodos:
	
	1.1) Isolation forest. Consiste em uma árvore de decisão que isola os *outliers* por meio de parâmetros detectados de forma aleatória.
	2.1) Local Outlier Factor (LOF). Consiste em um algoritmo que computa a densidade local para determinar os *data points* que divergem dela - e, portanto, apresentam baixa densidade -, e que serão considerados os *outliers*.
	
B. Colocar nossos nomes

C. Talvez detalhar sobre as coisas que falei em (A) dentro de cada readme de cada bloco?

D. dezz nuts


Validação Cruzada (k-fold):

Definição: método que divide o conjunto total de dados em k subconjuntos de tamanhos iguais. por meio desses conjuntos, um sub-conjunto é utilizado para teste, e o resto é utilizado para estimar parâmetros, a fim de testar a precisão/acurácia do modelo.

5 treinos para cada conjunto de parametro que estamos avaliando
cada bloco azul é um teste, os verdes são o treino
se o conjunto de dados não é homogêneo pode haver erros. -> exemplo: se no conjunto de 1000 pacientes há apenas 20 que sofreram X, é difícil utilizar esses valores do dataset (tipo valores anomalos?)



